#+title: (Binary)Logistic Classification
#+subtitle: supplement learning
#+date: <2020-09-27 Wed 11:00>
#+tags: python, bash, elisp, lisp, zoom
#+property: header-args:bash :results verbatim
#+property: header-args:elisp :exports both
#+property: header-args:ipython :session mglearn27-MLE :tangle "mglearn200927.py" :exports both

#+author: srolisp

* Basics
$Given: D = ((x_{1}, y_{1}), (x_{2}, y_{2}), \dots, (x_{d}, y_{d})),\ \ x_{i} \in \mathbb{R}^n, y_{i} \in \{0,1\}$

$Model: y_{i} \sim Bernoulli(\sigma(w^Tx))$ indep., (w = param.)\(\qquad\qquad\qquad Logistic\ fn:  \sigma(a) = \frac{1}{1+e^{-a}} \)

$MLE : w_{MLE} \in \underset{w}{arg\,max}$ $p(D|w)$ \\

\(\)
\begin{equation*}
\begin{split}
p(D|w) = \prod_{i=1}^{d} p(y_{i}|x_{i}, w) = \prod_{i=1}^{d} \alpha_{i}^{y_{i}}(1-\alpha_{i})^{1-y_{i}} \qquad\qquad\qquad \alpha_{i} = \sigma(w^{T}x_{i})
\end{split}
\end{equation*}
\(p(D|w)\)가 곱의 연산이라 더 진행하기 어려우므로 \(log\)를 취해 합의 연산으로 바꾸자. 

TODO: \(log\) 함수를 취할 수 있는 이유는 증가함수이므로 \(p(D|w)\) 가 최대가 되는 것은 \(log\ {p(D|w)}\) 가 최대가 되는 지점과 같다.

Loss 함수로 표현하기 위해 앞에 \(-\)를 취해서 최소함수로 변환시키자.

\begin{equation}
\mathscr{L}(w) = -log\ p(D|w) = -\sum_{i=1}^{d}\{y_{i}log\alpha_{i} + (1-y_{i})log(1-\alpha_{i})\}
\end{equation}

* Goal
\(\mathscr{L}(w)\)가 최소인 w 파라메터를 찾는 게 목표. \(\qquad w \in \mathbb{R}^n\)
* Pros

** Interpretable (main reason)
 $w_{0} + w_{1}feature_{1} + w_{2}feature_{2} + w_{3}feature_{3}$

** Small number of params $(n+1)$ $w_{0}$ 는 bias)

** Computationally eff. to estimate w (Newton's method)

** Multiclass

** Foundational

* Cons

** Not good as best performance methods

* \(w_{MLE}\)
\(log\alpha = log\sigma(w^{T}x) = log\frac{1}{1+e^{-w^{T}x}} = -log(1+e^{-w^{T}x})\)
\begin{equation} 
\begin{split}
\frac{\delta}{\delta w_{j}}log\alpha_{i} & = \frac{\delta}{\delta w_{j}} -log({1+e^{-w^{T}x_{i}}}) \\
                                         & = \frac{e^{-w^{T}x_{i}}}{1+e^{-w^{T}x_{i}}}x_{ij} \\
                                         & = (1 - \alpha_{i})x_{ij} 
\end{split}
\end{equation} 
\(log(1-\alpha) = log(1- \sigma(w^{T}x)) = log(1-\frac{1}{1+e^{-w^{T}x}}) = log\frac{e^{-w^{T}x}}{1+e^{-w^{T}x}} = -w^{T}x - log(1+e^{-w^{T}x})\)
\begin{equation}
\begin{split}
\frac{\delta}{\delta w_{j}}log(1-\alpha_{i}) & = \frac{\delta}{\delta w_{j}} (-w^{T}x_{i} - log(1+e^{-w^{T}x_{i}})) \\
                                             & = x_{ij} + \frac{\delta}{\delta w_{j}} - log(1+e^{-w^{T}x_{i}}) \\
\end{split}
\end{equation}
(2) 식을 활용해서,
\begin{equation}
\begin{split}
\frac{\delta}{\delta w_{j}}log(1-\alpha_{i}) & = x_{ij} + (1 - \alpha_{i})x_{ij} \\
                                             & = -\alpha_{i}x_{ij}
\end{split}
\end{equation}

\(\frac{\delta}{\delta w} \mathscr{L}(w) = 0\)을 만족하는 w_{MLE}
\begin{equation}
\mathscr{L}(w) = -\sum_{i=1}^{d}\{y_{i}log\alpha_{i} + (1-y_{i})log(1-\alpha_{i})\}
\end{equation}

\begin{equation}
\begin{split}
\frac{\delta}{\delta w_{j}} \mathscr{L}(w) & = -\sum_{i=1}^{d}\{y_{i}(1-a_{i})x_{ij} - (1-y_{i})\alpha_{i}x_{ij}\} \\
                                           & = -\sum_{i=1}^{d}\{y_{i}x_{ij} - \alpha_{i}x_{ij}\} \\
                                           & = \sum_{i=1}^{d}(\alpha_{i} - y_{i})x_{ij} \\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\frac{\delta}{\delta w_{j}} \mathscr{L}(w) & = \sum_{i=1}^{d}(\alpha_{i} - y_{i})x_{ij} \\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\frac{\delta}{\delta w_{1}} \mathscr{L}(w) & = \sum_{i=1}^{d}(\alpha_{i} - y_{i})x_{i1} \\
                                           & = (\alpha_{1}-y_{1})x_{11} + (\alpha_{2}-y_{2})x_{21} + \dots + (\alpha_{d}-y_{d})x_{d1} \\
\frac{\delta}{\delta w_{2}} \mathscr{L}(w) & = (\alpha_{1}-y_{1})x_{12} + (\alpha_{2}-y_{2})x_{22} + \dots + (\alpha_{d}-y_{d})x_{d2} \\
                                           & \vdots \\
\frac{\delta}{\delta w_{n}} \mathscr{L}(w) & = (\alpha_{1}-y_{1})x_{1n} + (\alpha_{2}-y_{2})x_{2n} + \dots + (\alpha_{d}-y_{d})x_{dn} \\
\end{split}
\end{equation}
풀어놓고 보니 이제 변환할 수 있을 것 같다. 익숙해지도록 연습하자

\(A = \begin{bmatrix}
                                           x_{11} & x_{12} & \dots & x_{1n} \\
                                           x_{21} & x_{22} & \dots & x_{2n} \\
                                                  &        & \vdots & \\
                                           x_{d1} & x_{d2} & \dots & x_{dn} \\
                                           \end{bmatrix} \\\)
\begin{equation}
\begin{split}
\frac{\delta}{\delta w^T} \mathscr{L}(w) & = \begin{bmatrix}
                                           \alpha_{1}-y_{1}  & \alpha_{2}-y_{2} & \alpha_{3}-y_{3} & \dots & \alpha_{d}-y_{d}
                                           \end{bmatrix}^T
                                           \begin{bmatrix}
                                           x_{11} & x_{12} & \dots & x_{1n} \\
                                           x_{21} & x_{22} & \dots & x_{2n} \\
                                                  &        & \vdots & \\
                                           x_{d1} & x_{d2} & \dots & x_{dn} \\
                                           \end{bmatrix} \\
                                       & = (a-y)^TA
\end{split}
\end{equation}
\(\frac{\delta^2}{\delta w_{j}\delta w_{k}} \mathscr{L}(w)\) 를 구해보자.


\begin{equation}
\begin{split}
\frac{\delta^2}{\delta w_{j}\delta w_{k}} \mathscr{L}(w) & = \frac{\delta}{\delta w_{k}}\sum_{i=1}^{d}(\alpha_{i} - y_{i})x_{ij} \\
                                                         & = \sum_{i=1}^{d}\frac{\delta}{\delta w_{k}}\{(\alpha_{i} - y_{i})x_{ij}\} \\
                                                         & = \sum_{i=1}^{d}x_{ij}\frac{\delta}{\delta w_{k}}\alpha_{i} \\

\end{split}
\end{equation}
\(\frac{\delta}{\delta w_{k}}\alpha_{i} \) 는,
\begin{equation}
\begin{split}
\frac{\delta}{\delta w_{k}} log\alpha_{i} & = x_{k}(1-\alpha_{i}) \\
\delta log\alpha_{i} & = x_{k}(1-\alpha_{i})\delta w_{k} \qquad\qquad\qquad\qquad \delta log\alpha_{i} = \frac{\delta \alpha_{i}}{\alpha_{i}} \\
\frac{\delta \alpha_{i}}{\alpha_{i}} & = x_{k}(1-\alpha_{i})\delta w_{k} \\
\frac{\delta \alpha_{i}}{\delta w_{k}} & = \alpha_{i}(1-\alpha_{i})x_{k} \\
\end{split}
\end{equation}
이렇게 전개해서 정리를 하던데, 나는 일일히 전개해서 구했다. 여튼, \\
(11)식에서 구한 미분 \(\frac{\delta}{\delta w_{k}}\alpha_{i}^T\) 을 (10)식에 대입하면, 
\begin{equation}
\begin{split}
\frac{\delta^2}{\delta w_{j}\delta w_{k}} \mathscr{L}(w) & = \sum_{i=1}^{d}x_{ij}\alpha_{i}(1-\alpha_{i})x_{ik} \\
\end{split}
\end{equation}
행렬로 표현하면 \(n\ x\ n\) 로 표현된다.
\begin{equation}
\begin{split}
\frac{\delta^2}{\delta w^2} \mathscr{L}(w) & = A^TBA \qquad\qquad\qquad B = \begin{bmatrix} \alpha_{1}(1-\alpha_{1}) & 0 & \dots &  & 0\\
                                                                                             0 & \alpha_{2}(1-\alpha_{2}) &  & & \vdots \\
                                                                                             \vdots & \ddots & \ddots & \ddots & 0 \\
                                                                                             0 & 0 &  & \dots & \alpha_{d}(1-\alpha_{d}) \\
                                                                                             \end{bmatrix}\\
\end{split}
\end{equation}


\begin{equation}
\begin{split}
A^TBA & = A^TB^{1/2}B^{1/2}A \\
      & = (B^{1/2}A)^T(B^{1/2}A)
\end{split}
\end{equation}
따라서, \(\mathscr{L}\) 은 convex 이다.
convex이므로 최소가 되는 지점을 newton method를 사용해서 구해보자.
\begin{equation}
\begin{split}
w_{t+1} & = w_{t} - H^{-1}g  \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\ \ \  H = A^TBA \\
                & \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad g = A^T(a-y) 수정\\
        & = w_{t} - (A^TBA)^{-1}A^T(a-y) \\
        & = (A^TBA)^{-1}\{(A^TBA)w_{t}-A^T(a-y)\} \\
        & = (A^TBA)^{-1}(A^TB)\{Aw_{t}-B^{-1}(a-y)\} \\
\end{split}
\end{equation}

* Implements
#+begin_src ipython :results output 
  import numpy as np
  from numpy.linalg import inv
  class LogisticRegression:

      def __init__(self, n_iters=1000):
          self.lr = lr
          self.n_iters = n_iters
          self.weights = None
          self.bias = None
          self.a = None

      def fit(self, X, y):
          # init parameters
          # n_samples, n_features = X.shape
          self.weights = np.zeros(X.shape[1])

          for _ in range(self.n_iters):
              wx = np.dot(X, self.weights)
              a = self._sigmoid(wx)
              one_a = np.array([1-a_i for a_i in a])

              B = np.diag(a * one_a)
              dldw = np.dot((a - y).T, X)
              d2ldw2 = np.dot(np.dot(X.T, B), X)
              inv_d2ldw2 = inv(d2ldw2)
              # update weights
              self.weights -= np.dot(inv_d2ldw2, dldw)
          self.a = a

      def predict(self, X):
          y_predicted = self._sigmoid(np.dot(X, self.weights))
          y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]
          return y_predicted_cls

      def result_a(self):
          return self.a

      def result_weights(self):
          return self.weights

      def _sigmoid(self, a):
           return 1 / (1 + np.exp(-a))

      def _print_debug(self, str, X):
           print(str + '\n', X.shape)
           print(X)
#+end_src

#+RESULTS:
Hessian 역행렬을 구해야하다보니 linear dependent 조건을 잘 걸러야 하네.
#+begin_src ipython :results output
  from sklearn.model_selection import train_test_split
  from sklearn import datasets
  import matplotlib.pyplot as plt

  bc = datasets.load_breast_cancer()
  X, y = bc.data[:,:21], bc.target

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)

  def accuracy(y_true, y_pred):
      accuracy = np.sum(y_true == y_pred) / len(y_true)
      return accuracy

  r = LogisticRegression()
  r.fit(X_train, y_train)
  predictions = r.predict(X_test)

  print("LR classification accuracy:", accuracy(y_test, predictions))

#+end_src

#+RESULTS:
: LR classification accuracy: 0.9298245614035088

#+begin_src ipython :results raw drawer
  x_3d = np.linspace(0, 40, 455)
  # print(len(x_3d))
  y_3d = np.linspace(0, 3000, 455)
  # print(len(y_3d))
  # x_3d, y_3d = np.meshgrid(x_3d, x_3d)
  z = r.result_a()
  # print(len(z))
  fig = plt.figure(figsize=(6,9))
  ax = fig.add_subplot(111, projection='3d')
  # ax.contour(x_3d, y_3d, z.reshape(35, 13))
  # ax.scatter(X_train[:,10],X_train[:,3],r.result_a())
  # ax.plot_surface(x_3d,y_3d,z.reshape(13, 35))
  ax.scatter(X_train[:,10],X_train[:,3],y_train, marker='.',c='red')
  ax.scatter(X_train[:,10],X_train[:,3],z)


  # ax.plot(x_3d,y_3d, z )
  ax.view_init(elev=5.,
  azim=320
  )

#+end_src

#+RESULTS:
:results:
# Out[598]:
[[file:./obipy-resources/w7IZ6E.png]]
:end:

#+begin_src ipython :results raw drawer
  plt.figure()
  plt.scatter(np.dot(X_train, r.result_weights()), r.result_a(), marker='.')
  plt.scatter(np.dot(X_train, r.result_weights()), y_train, marker='_', c='red', alpha=.3)
  plt.show()
#+end_src

#+RESULTS:
:results:
# Out[599]:
[[file:./obipy-resources/FpjogA.png]]
:end:
